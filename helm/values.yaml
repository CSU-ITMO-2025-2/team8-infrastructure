replicaCount: 1
app:
  replicaCount: 1
  containerPort: 8080
  image:
    repository: ghcr.io/csu-itmo-2025-2/team8-backend
    tag: sha-8d11c6b
    pullPolicy: IfNotPresent

  env:
    - name: KAFKA_SERVERS
      value: "kafka.team8-ns.svc:9092"

  resources:
    requests:
      cpu: 250m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 256Mi

  rollout:
    enabled: false
    analysisTemplate:
      enabled: false

vault:
  enabled: true
  server: "https://vault.kubepractice.ru"
  kvMount: "kv"
  kvVersion: "v1"
  kubernetesAuthMount: "kubernetes"
  role: "team8"
  envPath: "team8/backend"

worker:
  replicaCount: 1
  image:
    repository: ghcr.io/csu-itmo-2025-2/team-8-llm-worker/dummy-llm-worker
    tag: latest
    pullPolicy: IfNotPresent
  httpPort: 8080
  kafka:
    bootstrapServers: "kafka.team8-ns.svc:9092"
    topic: llm.chat.request
    consumerGroup: llm_worker
  env:
    - name: KAFKA_SERVERS
      value: "kafka.team8-ns.svc:9092"
  hfCache:
    enabled: true
    mountPath: /var/cache/huggingface
    sizeLimit: 10Gi
  probes:
    readiness:
      path: /health/ready
      initialDelaySeconds: 20
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
    liveness:
      path: /health/live
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 5
      failureThreshold: 5
    startup:
      enabled: true
      path: /health/live
      periodSeconds: 10
      failureThreshold: 18
  lifecycle:
    preStop:
      enabled: true
      path: /shutdown
  terminationGracePeriodSeconds: 60
  tmpDir:
    enabled: true
    mountPath: /tmp
    sizeLimit: ""
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    readOnlyRootFilesystem: true
  hpa:
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 60
  keda:
    enabled: true
    bootstrapServers: "kafka.team8-ns.svc:9092"
    topic: llm.chat.request
    consumerGroup: llm_worker
    lagThreshold: 10
    pollingInterval: 10
    cooldownPeriod: 60
    minReplicaCount: 1
    maxReplicaCount: 2
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
      scaleUp:
        policies:
          - type: Percent
            value: 200
            periodSeconds: 60
  resources:
    requests:
      cpu: 25m
      memory: 32Mi
    limits:
      cpu: 100m
      memory: 64Mi

kafka:
  replicaCount: 1
  image:
    repository: bitnamilegacy/kafka
    tag: 4.0.0-debian-12-r10
    pullPolicy: IfNotPresent
  service:
    brokerPort: 9092
    controllerPort: 9093
  config:
    nodeId: "0"
    processRoles: "controller,broker"
    listeners: "PLAINTEXT://:9092,CONTROLLER://:9093"
    advertisedListeners: "PLAINTEXT://kafka.team8-ns.svc.cluster.local:9092"
    listenerSecurityProtocolMap: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
    controllerQuorumVoters: "0@kafka.team8-ns.svc.cluster.local:9093"
    controllerListenerNames: "CONTROLLER"
    autoCreateTopicsEnable: "true"
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 700m
      memory: 1Gi
  persistence:
    enabled: false
    size: 10Gi
    storageClass: ""

frontend:
  enabled: true
  replicaCount: 1
  image:
    repository: ghcr.io/csu-itmo-2025-2/team8-frontend
    tag: sha-ccb0b0c
  containerPort: 4173
  env:
    - name: API_ROUTE
      value: app
  service:
    type: ClusterIP
    port: 4173
    targetPort: 4173
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 250m
      memory: 256Mi
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 3
    targetCPUUtilizationPercentage: 70m
    targetMemoryUtilizationPercentage: 200Mi

#migrations:
#  enabled: true
#  image: liquibase:4.31.1-alpine
#  url: "jdbc:postgresql://<postgres-host>:5432/<db>"
#  user: postgres
#  password: pass
#  resources:
#    requests:
#      cpu: 100m
#      memory: 256Mi
#    limits:
#      cpu: 250m
#      memory: 512Mi


imagePullSecrets:
  - name: ghcr-secret

nameOverride: ""

fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations: {}

podSecurityContext: {}

securityContext: {}

serviceBackend:
  type: ClusterIP
  port: 80

#serviceFront:
#  type: ClusterIP
#  port: 4173

ingress:
  enabled: true
  className: "nginx"
  hosts:
    - host: team8.kubepractice.ru
      paths:
        - path: /
          pathType: Prefix
  tls:
    - hosts:
        - team8.kubepractice.ru
      secretName: tls-secret
#istioIngress:
#  enabled: true
#  hosts:
#    - demo.kubepractice-istio.ru
#  gateway:
#    name: team8-app-gateway
#  tls:
#    enabled: true
#    secretName: demo-istio-tls
#    issuerRef:
#      name: letsencrypt-istio
#      kind: ClusterIssuer
#  redirectHttpToHttps: true
#resources:
#  limits:
#    cpu: 250m
#    memory: 256Mi
#  requests:
#    cpu: 100m
#    memory: 128Mi
#nodeSelector: {}
#tolerations: []
#affinity: {}
#podAntiAffinity:
#  enabled: true
#  topologyKey: kubernetes.io/hostname
podAntiAffinity:
  enabled: false

